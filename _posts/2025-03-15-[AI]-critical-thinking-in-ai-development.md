---
layout: post
title: "[Science] Critical Thinking in Science: Evaluating Claims and Evidence"
tags: [Critical Thinking, Evidence, Scientific Method, Evaluation]
thumbnail: /images/thumbnails/2025-03-15-[Science]-critical-thinking-in-science.png
---

Scientific literacy requires evaluating claims systematically. This article provides a framework for critical assessment of scientific evidence.

## The Hierarchy of Evidence

Not all evidence is equal. Ranked from strongest to weakest:

1. **Systematic reviews & meta-analyses**: Synthesize multiple studies
2. **Randomized controlled trials (RCTs)**: Gold standard for causation
3. **Cohort studies**: Follow groups over time
4. **Case-control studies**: Compare cases to controls retrospectively
5. **Cross-sectional studies**: Snapshot at one time point
6. **Case reports**: Individual observations
7. **Expert opinion**: No empirical data

Higher levels provide stronger evidence for causal claims.

## Evaluating Individual Studies

### Questions to Ask

**About the source**:
- Is it peer-reviewed?
- What journal published it?
- Who funded the research?
- Do authors have conflicts of interest?

**About the methods**:
- Is the sample size adequate?
- Is the design appropriate for the question?
- Are controls appropriate?
- How was randomization/blinding implemented?

**About the results**:
- Are effect sizes reported (not just p-values)?
- Are confidence intervals provided?
- Are results practically significant?
- Do conclusions follow from data?

### Red Flags

| Red Flag | Concern |
|----------|---------|
| No peer review | Lacks quality control |
| No methods section | Cannot evaluate validity |
| Only p-values, no effect sizes | May be trivial effect |
| Claims beyond data | Overgeneralization |
| Single study, strong claims | Unreplicated |
| Conflicts of interest | Bias motivation |

## Common Logical Fallacies

### Post Hoc Ergo Propter Hoc

"After this, therefore because of this"

```
Observation: I took vitamin C, then my cold went away
Fallacy: Therefore vitamin C cured my cold
Reality: Colds resolve naturally; correlation ≠ causation
```

### Appeal to Nature

"Natural = good, synthetic = bad"

```
Fallacy: This treatment is natural, so it's safe
Reality: Many natural substances are toxic (arsenic, hemlock)
```

### Anecdotal Evidence

"It worked for me"

```
Fallacy: My experience proves it works
Reality: Individual cases don't establish general effects
        (placebo effect, regression to mean, coincidence)
```

### Appeal to Authority

"Expert X said so"

```
Fallacy: This scientist believes it, so it's true
Reality: Scientists can be wrong; evidence matters, not credentials
```

### Cherry Picking

Selecting favorable evidence

```
Fallacy: These 3 studies support my view
Reality: 30 other studies contradict it
```

## Evaluating Systematic Reviews

### Quality Indicators

**Good systematic review**:
- Pre-registered protocol
- Comprehensive search strategy
- Clear inclusion/exclusion criteria
- Risk of bias assessment
- Heterogeneity analysis
- Publication bias assessment

**PRISMA Flow Diagram**:
```
Records identified → Screened → Eligible → Included
      ↓                ↓           ↓
   Excluded        Excluded    Excluded
   (duplicates)    (irrelevant) (quality)
```

### Meta-Analysis Interpretation

**Forest plot reading**:
- Each line = one study
- Diamond = pooled estimate
- Line crossing 0 = not significant
- Narrow CI = precise estimate

**Heterogeneity (I²)**:
- 0-25%: Low heterogeneity
- 25-75%: Moderate
- >75%: High (results vary significantly)

## The Replication Principle

Single studies are unreliable. Require:
- Independent replication
- Different samples
- Different researchers
- Consistent findings

**Trust hierarchy**:
```
Multiple independent replications > Single large study >
Multiple studies by same group > Single study
```

## Practical Framework

When encountering a scientific claim:

```
1. Source check
   [ ] Peer-reviewed journal?
   [ ] Reputable authors/institutions?
   [ ] Conflicts of interest disclosed?

2. Methods check
   [ ] Appropriate design for question?
   [ ] Adequate sample size?
   [ ] Proper controls?

3. Results check
   [ ] Effect size meaningful?
   [ ] Replicated elsewhere?
   [ ] Consistent with existing knowledge?

4. Interpretation check
   [ ] Conclusions match data?
   [ ] Limitations acknowledged?
   [ ] Alternative explanations considered?
```

## Conclusion

Critical evaluation of scientific evidence is a skill that improves with practice. Default to skepticism, especially for extraordinary claims, and update beliefs based on the weight of evidence rather than individual studies.
